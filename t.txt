================================================
FILE: ./requirements.txt
================================================
tensorflow
numpy
mlflow
flask


================================================
FILE: ./app.py
================================================
from flask import Flask, request, jsonify
import tensorflow as tf
from tensorflow import keras
import numpy as np

app = Flask(__name__)

# Chargement du modèle Keras
model = keras.models.load_model('mnist_model.h5')

@app.route('/predict', methods=['POST'])
def predict():
    data = request.json
    # Vérification des données
    if 'image' not in data:
        return jsonify({'error': 'No image provided'}), 400

    image_data = np.array(data['image'])
    
    # Assurez-vous que l'image est au bon format (1, 784) et normalisée
    try:
        image_data = image_data.reshape(1, 784)
        image_data = image_data.astype("float32") / 255.0
    except ValueError:
        return jsonify({'error': 'Invalid image format. Expected a list of 784 numbers.'}), 400

    prediction = model.predict(image_data)
    predicted_class = np.argmax(prediction, axis=1)[0]
    
    return jsonify({
        'prediction': int(predicted_class),
        'probabilities': prediction.tolist()
    })

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)


================================================
FILE: ./README.md
================================================
# TP2 : Amélioration des Réseaux de Neurones Profonds

Ce projet est la suite du TP1 et se concentre sur les techniques avancées pour améliorer la performance et la robustesse des modèles de Deep Learning.

## Objectifs et Techniques Explorées

*   **Diagnostic de Performance** : Analyse du biais et de la variance en utilisant des ensembles d'entraînement et de validation distincts.
*   **Régularisation** : Mise en œuvre de la régularisation L2 et du Dropout pour combattre le surapprentissage.
*   **Optimisation Avancée** : Comparaison des performances des optimiseurs Adam, RMSprop et SGD avec momentum.
*   **Normalisation** : Utilisation de la Batch Normalization pour accélérer et stabiliser l'entraînement.

Toutes les expériences sont suivies et comparées à l'aide de **MLflow**.

## Structure du Projet

```
.
├── run_experiments.py  # Script principal pour lancer toutes les expériences
├── requirements.txt    # Dépendances Python
└── report_tp2.pdf      # Rapport résumant les concepts et les résultats
```

## Comment l'utiliser ?

### Prérequis

*   Python 3.8+
*   Avoir installé les dépendances listées dans `requirements.txt`.

### 1. Installation des dépendances

Clonez le dépôt et installez les bibliothèques nécessaires :

```bash
git clone https://github.com/QByteSeeker/TP_DL.git
cd TP_DL
checkout tp2

python3 -m venv venv # Si aucun environnement virtuel n'est défini
source venv/bin/activate
pip install -r requirements.txt
```

### 2. Lancer les Expériences

Exécutez le script principal. Cela va entraîner séquentiellement les différents modèles et enregistrer les résultats dans MLflow.
```bash
python run_experiments.py
```

### 3. Visualiser les Résultats

Pour comparer les performances des différents modèles, lancez l'interface utilisateur de MLflow dans votre terminal :
```bash
mlflow ui
```
Ouvrez votre navigateur à l'adresse `http://127.0.0.1:5000` pour analyser les courbes d'apprentissage, les métriques et les paramètres de chaque exécution.


================================================
FILE: ./Dockerfile
================================================
# Utiliser une image de base Python
FROM python:3.9-slim

# Définir le répertoire de travail
WORKDIR /app

# Copier le fichier des dépendances et les installer
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copier le reste de l'application
COPY . .

# Exposer le port de l'application Flask
EXPOSE 5000

# Commande pour démarrer l'application
CMD ["python", "app.py"]


================================================
FILE: ./run_experiments.py
================================================
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import regularizers
import numpy as np
import mlflow
import mlflow.tensorflow

# Préparation des données
def load_and_prepare_data():
    """Charge et prépare les données MNIST en ensembles d'entraînement, validation et test."""
    (x_train_full, y_train_full), (x_test, y_test) = keras.datasets.mnist.load_data()

    # Normalisation
    x_train_full = x_train_full.astype("float32") / 255.0
    x_test = x_test.astype("float32") / 255.0

    # Redimensionnement
    x_train_full = x_train_full.reshape(-1, 784)
    x_test = x_test.reshape(-1, 784)

    # Création des ensembles de validation (dev) et d'entraînement
    x_val = x_train_full[54000:]
    y_val = y_train_full[54000:]
    x_train = x_train_full[:54000]
    y_train = y_train_full[:54000]
    
    return (x_train, y_train), (x_val, y_val), (x_test, y_test)

# Fonction pour construire le modèle
def build_model(use_l2=False, use_dropout=False, use_batch_norm=False):
    """Construit un modèle Keras avec des options pour la régularisation et la batch norm."""
    
    layers = [keras.layers.Input(shape=(784,))]
    
    # Couche dense avec régularisation L2 optionnelle
    l2_reg = regularizers.l2(0.001) if use_l2 else None
    layers.append(keras.layers.Dense(512, activation='relu', kernel_regularizer=l2_reg))

    # Batch Normalization optionnelle (Exercice 2.4)
    if use_batch_norm:
        layers.append(keras.layers.BatchNormalization())
    
    # Dropout optionnel
    if use_dropout:
        layers.append(keras.layers.Dropout(0.2)) # Taux de dropout du TP1
        
    layers.append(keras.layers.Dense(10, activation='softmax'))
    
    model = keras.Sequential(layers)
    return model

# Script principal pour lancer les expériences
if __name__ == "__main__":
    (x_train, y_train), (x_val, y_val), (x_test, y_test) = load_and_prepare_data()

    # Expérience 1: Analyse Biais/Variance (Modèle de base)
    print("\n--- Démarrage de l'expérience 1: Modèle de base ---")
    with mlflow.start_run(run_name="Base_Model_Bias_Variance"):
        model_base = build_model()
        model_base.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
        
        history = model_base.fit(
            x_train, y_train,
            epochs=5,
            batch_size=128,
            validation_data=(x_val, y_val)
        )
        
        test_loss, test_acc = model_base.evaluate(x_test, y_test)
        mlflow.log_params({"regularization": "none", "batch_norm": False, "optimizer": "adam"})
        mlflow.log_metric("test_accuracy", test_acc)
        mlflow.log_metric("final_val_accuracy", history.history['val_accuracy'][-1])

    # Expérience 2: Application de la Régularisation (L2 + Dropout)
    print("\n--- Démarrage de l'expérience 2: Modèle avec Régularisation ---")
    with mlflow.start_run(run_name="Regularized_Model"):
        model_reg = build_model(use_l2=True, use_dropout=True)
        model_reg.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
        
        history = model_reg.fit(
            x_train, y_train,
            epochs=5,
            batch_size=128,
            validation_data=(x_val, y_val)
        )
        
        test_loss, test_acc = model_reg.evaluate(x_test, y_test)
        mlflow.log_params({"regularization": "L2+Dropout", "batch_norm": False, "optimizer": "adam"})
        mlflow.log_metric("test_accuracy", test_acc)
        mlflow.log_metric("final_val_accuracy", history.history['val_accuracy'][-1])
        
    # Expérience 3: Comparaison des Optimiseurs
    print("\n--- Démarrage de l'expérience 3: Comparaison des Optimiseurs ---")
    optimizers = {
        'SGD_with_momentum': keras.optimizers.SGD(learning_rate=0.01, momentum=0.9),
        'RMSprop': 'rmsprop',
        'Adam': 'adam'
    }

    for opt_name, optimizer_instance in optimizers.items():
        with mlflow.start_run(run_name=f"Optimizer_Comparison_{opt_name}"):
            print(f"  Entraînement avec l'optimiseur : {opt_name}")
            # On utilise le modèle régularisé comme base de comparaison
            model_opt = build_model(use_l2=True, use_dropout=True)
            model_opt.compile(optimizer=optimizer_instance, loss='sparse_categorical_crossentropy', metrics=['accuracy'])
            
            history = model_opt.fit(
                x_train, y_train,
                epochs=5,
                batch_size=128,
                validation_data=(x_val, y_val),
                verbose=0 # Moins de logs pour la boucle
            )
            
            test_loss, test_acc = model_opt.evaluate(x_test, y_test, verbose=0)
            print(f"  Accuracy finale pour {opt_name}: {test_acc:.4f}")
            mlflow.log_params({"regularization": "L2+Dropout", "batch_norm": False, "optimizer": opt_name})
            mlflow.log_metric("test_accuracy", test_acc)
            mlflow.log_metric("final_val_accuracy", history.history['val_accuracy'][-1])
    
    # Expérience 4: Ajout de la Batch Normalization
    print("\n--- Démarrage de l'expérience 4: Modèle avec Batch Normalization ---")
    with mlflow.start_run(run_name="Model_With_Batch_Norm"):
        model_bn = build_model(use_l2=True, use_dropout=True, use_batch_norm=True)
        model_bn.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
        
        history = model_bn.fit(
            x_train, y_train,
            epochs=5,
            batch_size=128,
            validation_data=(x_val, y_val)
        )
        
        test_loss, test_acc = model_bn.evaluate(x_test, y_test)
        mlflow.log_params({"regularization": "L2+Dropout", "batch_norm": True, "optimizer": "adam"})
        mlflow.log_metric("test_accuracy", test_acc)
        mlflow.log_metric("final_val_accuracy", history.history['val_accuracy'][-1])

    print("\nToutes les expériences sont terminées. Lancez 'mlflow ui' pour voir les résultats.")


